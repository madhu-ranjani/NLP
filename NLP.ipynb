{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGwLmbQHRTGN"
      },
      "outputs": [],
      "source": [
        "text=\" My name is Madhuranjani\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1io6gi4RR_ZM",
        "outputId": "64a24a8f-8a5b-45d6-e40b-c08aef6d0565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My', 'name', 'is', 'Madhuranjani']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\" My name is Madhuranjani. I am studying in bitsathy\""
      ],
      "metadata": {
        "id": "qKNt-QXaST3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.split('.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIjdX2JWSnTq",
        "outputId": "782057ec-4d9b-4257-be90-3eb9eefac13e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' My name is Madhuranjani', ' I am studying in bitsathy']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REGULAR   EXPRESSION(USING REGEX)\n"
      ],
      "metadata": {
        "id": "4hCq2j7NSjqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "dkBPzw9lTYjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Who are you? My name is Madhuranjani. I am studying in bitsathy.\""
      ],
      "metadata": {
        "id": "tM90373ATHPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text= re.compile('[?.]').split(text)"
      ],
      "metadata": {
        "id": "ZehjdwiMTvNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj0iA1qAUN2e",
        "outputId": "f51e98c4-9c58-4773-dad5-c7266831a612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Who are you', ' My name is Madhuranjani', ' I am studying in bitsathy', '']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "USING NLTK"
      ],
      "metadata": {
        "id": "Wxrt7eGTU74u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "d3w2SonvU_I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk .tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "dDmrlo0ZUPI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJOGlIQdVPeu",
        "outputId": "3c769c69-8996-4f8b-ba0e-8b41b97e65e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"Who are you? My name is Madhuranjani. I am studying in bitsathy.\""
      ],
      "metadata": {
        "id": "HLoKj-IjVjhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgLfVU2JVWU7",
        "outputId": "1d2e660f-71e0-444a-dd9f-6a7565d07869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Who',\n",
              " 'are',\n",
              " 'you',\n",
              " '?',\n",
              " 'My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Madhuranjani',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'studying',\n",
              " 'in',\n",
              " 'bitsathy',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ASSIGNMENT -3 TOKENIZATION FUNCTIONS"
      ],
      "metadata": {
        "id": "uJCntAgEWNLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk .tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "Eu68m8rAVupL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHD9HC-SWdt9",
        "outputId": "413f51fb-597b-42a3-e2fa-175a10f86749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Who are you?', 'My name is Madhuranjani.', 'I am studying in bitsathy.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "sentence = \"It won't be done until you start\"\n",
        "tokenizer=TreebankWordTokenizer()\n",
        "tokenizer.tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uusHf_O3d6IY",
        "outputId": "543115a1-f1b5-49b3-bd30-d53096b1c26c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It', 'wo', \"n't\", 'be', 'done', 'until', 'you', 'start']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "sentence = \"I can't allow you to go home early!\"\n",
        "wordpunct_tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytG4AfwOfxUN",
        "outputId": "ffb63f5c-dfd6-4093-a8f7-3113ed0d9830"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'can', \"'\", 't', 'allow', 'you', 'to', 'go', 'home', 'early', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HwOyAb-f9Ax",
        "outputId": "5237863e-b39e-4c4b-d6d7-ac0b85194b86"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "sentence = \"I'm Madhuranjani. Im currently pursuing BE CSE\"\n",
        "tokenizer = MWETokenizer()\n",
        "tokenizer.add_mwe(('BE','CSE'))\n",
        "tokenizer.tokenize(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl34ULxfgEfN",
        "outputId": "24ab151e-aa93-4b64-a79a-5c23caca4a48"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', \"'m\", 'Madhuranjani', '.', 'Im', 'currently', 'pursuing', 'BE_CSE']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEMMING"
      ],
      "metadata": {
        "id": "agnrMD5BWrhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "9JplPEDaWi-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter= PorterStemmer()"
      ],
      "metadata": {
        "id": "A2hVyL4uXJAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=['wait','waiting','waited','waits']"
      ],
      "metadata": {
        "id": "AjzOt6e2XTsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "        print(word,\"-->\",porter.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZLyihtVXds4",
        "outputId": "e251e87a-4528-4aa4-f864-78c1a09b206a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wait --> wait\n",
            "waiting --> wait\n",
            "waited --> wait\n",
            "waits --> wait\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem  import SnowballStemmer"
      ],
      "metadata": {
        "id": "zq4SqSXeYAJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowball=SnowballStemmer(language=\"english\")"
      ],
      "metadata": {
        "id": "nSURKZ6lZqUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "           print(word,\"-->\",snowball.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPjWj2jJZgrD",
        "outputId": "5398906e-999c-4d79-ab48-0850ad9e2a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wait --> wait\n",
            "waiting --> wait\n",
            "waited --> wait\n",
            "waits --> wait\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "lancaster = LancasterStemmer()\n",
        "words = ['eating','eats','eaten','puts','putting']\n",
        "for word in words:\n",
        "    print(word,\"--->\",lancaster.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQWM8Fh3fDgr",
        "outputId": "3e4624af-1ee5-49f7-f881-39c3114236ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating ---> eat\n",
            "eats ---> eat\n",
            "eaten ---> eat\n",
            "puts ---> put\n",
            "putting ---> put\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "words = ['mass','was','bee','computer','advisable']\n",
        "for word in words:\n",
        "    print(word,\"--->\",regexp.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXvJVSxxfESR",
        "outputId": "129e117a-e9b1-4551-c573-309324f965d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mass ---> mas\n",
            "was ---> was\n",
            "bee ---> bee\n",
            "computer ---> computer\n",
            "advisable ---> advis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LEMMATIZATION\n"
      ],
      "metadata": {
        "id": "tMlMJk6Zgbre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob, Word"
      ],
      "metadata": {
        "id": "JRlMKIljglpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9YKLEvlgrBd",
        "outputId": "91fa480f-8acc-425e-e35f-07d3e01eac8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "listofwords=['waiting','plays','files','borrowed']"
      ],
      "metadata": {
        "id": "0cX9FtKAhC3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in listofwords:\n",
        "    w= Word(word)\n",
        "    print(word+'-->'+ w.lemmatize())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gv6d5ZmhSJK",
        "outputId": "590bc683-53bd-45b2-abd5-473257d5693c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "waiting-->waiting\n",
            "plays-->play\n",
            "files-->file\n",
            "borrowed-->borrowed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "6K_QrQXqht4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wn1=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "Z-yvgC6niJXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for words in listofwords:\n",
        "  \n",
        "    print(words+'-->'+ wn1.lemmatize(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9Zlwjf9iTP2",
        "outputId": "ed6a8043-a991-49c1-f849-4b8c8f198077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "waiting-->waiting\n",
            "plays-->play\n",
            "files-->file\n",
            "borrowed-->borrowed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        " \n",
        "doc = nlp(u'the bats saw the cats with best stripes hanging upside down by their feet')\n",
        "\n",
        "tokens = []\n",
        "for token in doc:\n",
        "    tokens.append(token)\n",
        "print(tokens)\n",
        "\n",
        "lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n",
        "print(lemmatized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukpSZLGzgaDB",
        "outputId": "58650685-7f0d-40e2-e00d-d0fdbd295f32"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[the, bats, saw, the, cats, with, best, stripes, hanging, upside, down, by, their, feet]\n",
            "the bat see the cat with good stripe hang upside down by -PRON- foot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pattern"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUK_0f87g0HG",
        "outputId": "0f1dc4f8-e14d-4f39-a4d1-2ce9331c2a73"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pattern\n",
            "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.2 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pattern) (0.16.0)\n",
            "Collecting backports.csv\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Collecting mysqlclient\n",
            "  Downloading mysqlclient-2.1.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from pattern) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pattern) (4.2.6)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 8.7 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "  Downloading pdfminer.six-20220319-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 26.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pattern) (3.2.5)\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 42.3 MB/s \n",
            "\u001b[?25hCollecting cherrypy\n",
            "  Downloading CherryPy-18.6.1-py2.py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 20.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pattern) (2.23.0)\n",
            "Collecting portend>=2.1.1\n",
            "  Downloading portend-3.1.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting jaraco.collections\n",
            "  Downloading jaraco.collections-3.5.1-py3-none-any.whl (10 kB)\n",
            "Collecting cheroot>=8.2.1\n",
            "  Downloading cheroot-8.6.0-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 46.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (8.12.0)\n",
            "Collecting zc.lockfile\n",
            "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->pattern) (1.15.0)\n",
            "Collecting jaraco.functools\n",
            "  Downloading jaraco.functools-3.5.0-py3-none-any.whl (7.0 kB)\n",
            "Collecting tempora>=1.8\n",
            "  Downloading tempora-5.0.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2022.1)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Collecting jaraco.classes\n",
            "  Downloading jaraco.classes-3.2.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting jaraco.text\n",
            "  Downloading jaraco.text-3.7.0-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (5.7.1)\n",
            "Collecting jaraco.context>=4.1\n",
            "  Downloading jaraco.context-4.1.1-py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->jaraco.text->jaraco.collections->cherrypy->pattern) (3.8.0)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-37.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 56.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (3.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->pattern) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->pattern) (2.21)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2021.10.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zc.lockfile->cherrypy->pattern) (57.4.0)\n",
            "Building wheels for collected packages: pattern, mysqlclient, python-docx, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332721 sha256=0f26e739b19c6a22a2dc9c6902404dee531a9d7756f0471988bf58bd67847c76\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/1f/4e/9b67afd2430d55dee90bd57618dd7d899f1323e5852c465682\n",
            "  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.1.0-cp37-cp37m-linux_x86_64.whl size=99978 sha256=198828aabdd3cd4f0519e9e2a2f3b0456ada0613c943408649e361f394a81491\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/d4/df/08cd6e1fa4a8691b268ab254bd0fa589827ab5b65638c010b4\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=1a94a7a0978b294c1223590c20796692d31484becb9cf37d0d84c0ad8985391b\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=5c6ac1d7f3088705f732ef6063f3c8c1353c01c21d85552ee34f287c9eecf938\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built pattern mysqlclient python-docx sgmllib3k\n",
            "Installing collected packages: jaraco.functools, jaraco.context, tempora, jaraco.text, jaraco.classes, zc.lockfile, sgmllib3k, portend, jaraco.collections, cryptography, cheroot, python-docx, pdfminer.six, mysqlclient, feedparser, cherrypy, backports.csv, pattern\n",
            "Successfully installed backports.csv-1.0.7 cheroot-8.6.0 cherrypy-18.6.1 cryptography-37.0.0 feedparser-6.0.8 jaraco.classes-3.2.1 jaraco.collections-3.5.1 jaraco.context-4.1.1 jaraco.functools-3.5.0 jaraco.text-3.7.0 mysqlclient-2.1.0 pattern-3.6 pdfminer.six-20220319 portend-3.1.0 python-docx-0.8.11 sgmllib3k-1.0.0 tempora-5.0.1 zc.lockfile-2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pattern\n",
        "from pattern.en import lemma, lexeme\n",
        "from pattern.en import parse\n",
        " \n",
        "sentence = \"the bats saw the cats with best stripes hanging upside down by their feet\"\n",
        " \n",
        "lemmatized_sentence = \" \".join([lemma(word) for word in sentence.split()])\n",
        "print(\"Lemmatized sentence --->\",lemmatized_sentence)\n",
        "\n",
        "all_lemmas_for_each_word = [lexeme(wd) for wd in sentence.split()]\n",
        "print(all_lemmas_for_each_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI3FXl0Vg53h",
        "outputId": "5142a692-5a35-4f81-cf59-c61e62220cc6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized sentence ---> the bat see the cat with best stripe hang upside down by their feet\n",
            "[['the', 'thes', 'thing', 'thed'], ['bat', 'bats', 'batting', 'batted'], ['see', 'sees', 'seeing', 'saw', 'seen'], ['the', 'thes', 'thing', 'thed'], ['cat', 'cats', 'catting', 'catted'], ['with', 'withs', 'withing', 'withed'], ['best', 'bests', 'besting', 'bested'], ['stripe', 'stripes', 'striping', 'striped'], ['hang', 'hangs', 'hanging', 'hung'], ['upside', 'upsides', 'upsiding', 'upsided'], ['down', 'downs', 'downing', 'downed'], ['by', 'bies', 'bying', 'bied'], ['their', 'theirs', 'theiring', 'theired'], ['feet', 'feets', 'feeting', 'feeted']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import lemmatize\n",
        "sentence = \"the bats saw the cats with best stripes hanging upside down by their feet\"\n",
        "lemmatized_sentence = [word.decode('utf-8').split('.')[0] for word in lemmatize(sentence)]\n",
        "print(lemmatized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzM8pGrQg-pp",
        "outputId": "7fb594a9-bf24-4092-82b1-6ffb45af29bb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bat/NN', 'see/VB', 'cat/NN', 'best/JJ', 'stripe/NN', 'hang/VB', 'upside/RB', 'foot/NN']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm"
      ],
      "metadata": {
        "id": "WGo1wSmOinH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp= en_core_web_sm.load()"
      ],
      "metadata": {
        "id": "c5NKIpkUutos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent1=nlp(\"How Can I check my account balance\")\n",
        "sent2=nlp(\"I want to check my account balance\")\n",
        "print(sent2.similarity(sent1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paFk3bm1uzpx",
        "outputId": "4d68d074-8d67-4d85-e9f6-64a15f1010aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7938082248656653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY-5JzssvRpP",
        "outputId": "d59e32ff-2d57-4906-ec1f-d2d6707eac8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk \n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "XAQLoulawDxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9iW5pfqwXJR",
        "outputId": "0b2da42b-4e8d-4bca-9674-8ae5a4d53335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj4v5rZTwd_g",
        "outputId": "1624d8d2-7a50-4042-c265-f87b57fd15f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words=set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "MsPhF1AawkzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokens_1=word_tokenize(\"How can I check my account balance\")\n",
        "word_tokens_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3-zcUbywxwh",
        "outputId": "5236cfa3-d124-47f1-b34b-f9feffbe390b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How', 'can', 'I', 'check', 'my', 'account', 'balance']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_sent_1=[ w for w in word_tokens_1 if w.lower()  in stop_words]\n",
        "new_sent_1=' '.join(str(elem) for elem in new_sent_1)"
      ],
      "metadata": {
        "id": "eW9GGSSkxPgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_sent_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRLNCo8FyNUl",
        "outputId": "3059dc6b-8cc5-4796-b544-cfea81cbc86d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How can I my\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_sent_1=[ w for w in word_tokens_1 if w.lower() not in stop_words]\n",
        "new_sent_1=' '.join(str(elem) for elem in new_sent_1)\n",
        "print(new_sent_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDbLiJquyUlL",
        "outputId": "39128b5f-e549-4691-9c5f-69f18c6dae6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check account balance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens_2=word_tokenize(\"I want to  check my account balance\")\n",
        "new_sent_2=[ w for w in word_tokens_2 if w.lower() not in stop_words]\n",
        "new_sent_2=' '.join(str(elem) for elem in new_sent_2)\n",
        "print(new_sent_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0kRv0oMym0J",
        "outputId": "3cdfc1cd-2bfa-497b-b7b0-e95b08fd6d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "want check account balance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_sent_1=nlp(new_sent_1)\n",
        "new_sent_2=nlp(new_sent_2)\n",
        "print(new_sent_2.similarity(new_sent_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vM0KYKey5J7",
        "outputId": "59c8ff30-46c7-45c6-e942-aac89e1b8a5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9003613092570767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zVxCA3Wq0bfD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}